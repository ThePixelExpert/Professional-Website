---
phase: 04-production-infrastructure
plan: 06
type: execute
wave: 4
depends_on: ["04-03", "04-04", "04-05"]
files_modified:
  - production/apply-migrations.sh
  - docs/PRODUCTION_SETUP.md
autonomous: false

user_setup:
  - service: google_oauth
    why: "Admin authentication requires separate production OAuth app"
    env_vars:
      - name: GOTRUE_EXTERNAL_GOOGLE_CLIENT_ID
        source: "Google Cloud Console -> APIs & Services -> Credentials"
      - name: GOTRUE_EXTERNAL_GOOGLE_SECRET
        source: "Google Cloud Console -> APIs & Services -> Credentials"
    dashboard_config:
      - task: "Create OAuth 2.0 Client ID for production"
        location: "Google Cloud Console -> APIs & Services -> Credentials -> Create Credentials -> OAuth client ID"
      - task: "Add authorized redirect URI"
        location: "OAuth client ID settings -> Authorized redirect URIs: https://supabase.edwardstech.dev/auth/v1/callback"
  - service: cloudflare
    why: "DNS must point to VM for SSL certificates"
    dashboard_config:
      - task: "Create A record for supabase.edwardstech.dev"
        location: "Cloudflare Dashboard -> DNS -> Add record -> Type A, Name supabase, Content <VM_IP>"

must_haves:
  truths:
    - "Migrations can be applied to production database"
    - "OAuth authentication works in production"
    - "Frontend can connect to production Supabase"
    - "All services are healthy and accessible via HTTPS"
  artifacts:
    - path: "production/apply-migrations.sh"
      provides: "Migration deployment script"
      contains: "psql"
    - path: "docs/PRODUCTION_SETUP.md"
      provides: "Production configuration guide"
      contains: "OAuth"
  key_links:
    - from: "production/apply-migrations.sh"
      to: "supabase/migrations/"
      via: "Migration files applied to production DB"
      pattern: "migrations/"
    - from: "Frontend"
      to: "https://supabase.edwardstech.dev"
      via: "REACT_APP_SUPABASE_URL"
      pattern: "supabase.edwardstech.dev"
---

<objective>
Create migration deployment script and complete production setup with OAuth configuration and verification.

Purpose: This final plan connects all the infrastructure pieces, applies database migrations, configures OAuth, and verifies the complete production stack works end-to-end.

Output: Migration script, production setup documentation, and verified working production environment
</objective>

<execution_context>
@/home/logan/.claude/get-shit-done/workflows/execute-plan.md
@/home/logan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-production-infrastructure/04-RESEARCH.md
@supabase/config.toml

# Migration files to apply
@supabase/migrations/20260128000001_initial_schema.sql
@supabase/migrations/20260128000002_add_triggers.sql
@supabase/migrations/20260129000001_user_roles.sql
@supabase/migrations/20260129000002_auth_hook.sql
@supabase/migrations/20260129000003_customer_orders_link.sql
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create migration deployment script</name>
  <files>production/apply-migrations.sh</files>
  <action>
Create production/apply-migrations.sh that applies local migrations to production database.

```bash
#!/bin/bash
# Apply Migrations to Production Supabase
# Usage: ./apply-migrations.sh [migration-file]
#
# Without arguments: applies all migrations in order
# With argument: applies specific migration file

set -e

MIGRATIONS_DIR="../supabase/migrations"
DB_CONTAINER="supabase-db"

# Colors
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m'

log_info() { echo -e "${GREEN}[INFO]${NC} $1"; }
log_warn() { echo -e "${YELLOW}[WARN]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

# Get script directory (for relative paths)
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MIGRATIONS_PATH="$SCRIPT_DIR/$MIGRATIONS_DIR"

# Verify migrations directory exists
if [ ! -d "$MIGRATIONS_PATH" ]; then
    log_error "Migrations directory not found: $MIGRATIONS_PATH"
    log_info "Ensure you're running from the production/ directory"
    exit 1
fi

# Verify db container is running
if ! docker ps --format '{{.Names}}' | grep -q "^${DB_CONTAINER}$"; then
    log_error "Database container '$DB_CONTAINER' is not running"
    log_info "Start Supabase first: ./deploy.sh start"
    exit 1
fi

# Apply single migration
apply_migration() {
    local migration_file="$1"
    local filename=$(basename "$migration_file")

    log_info "Applying: $filename"

    # Execute migration
    docker exec -i $DB_CONTAINER psql -U postgres postgres < "$migration_file"

    if [ $? -eq 0 ]; then
        log_info "Success: $filename"
    else
        log_error "Failed: $filename"
        exit 1
    fi
}

# Apply all migrations
apply_all() {
    log_warn "This will apply ALL migrations to production!"
    log_warn "Migrations are NOT idempotent - only run once!"
    echo ""

    # List migrations
    log_info "Migrations to apply:"
    for f in "$MIGRATIONS_PATH"/*.sql; do
        echo "  - $(basename $f)"
    done
    echo ""

    read -p "Continue? (y/N) " -n 1 -r
    echo

    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log_info "Cancelled"
        exit 0
    fi

    # Create backup first
    log_info "Creating backup before migrations..."
    ./backup.sh backup

    # Apply migrations in order
    for migration in $(ls "$MIGRATIONS_PATH"/*.sql | sort); do
        apply_migration "$migration"
    done

    log_info "All migrations applied successfully!"
}

# Apply specific migration
apply_one() {
    local migration_name="$1"
    local migration_file="$MIGRATIONS_PATH/$migration_name"

    if [ ! -f "$migration_file" ]; then
        # Try adding .sql extension
        migration_file="$MIGRATIONS_PATH/${migration_name}.sql"
    fi

    if [ ! -f "$migration_file" ]; then
        log_error "Migration not found: $migration_name"
        log_info "Available migrations:"
        ls "$MIGRATIONS_PATH"/*.sql | xargs -n1 basename
        exit 1
    fi

    log_warn "This will apply migration to production!"
    log_warn "File: $(basename $migration_file)"
    read -p "Continue? (y/N) " -n 1 -r
    echo

    if [[ ! $REPLY =~ ^[Yy]$ ]]; then
        log_info "Cancelled"
        exit 0
    fi

    # Create backup first
    log_info "Creating backup before migration..."
    ./backup.sh backup

    apply_migration "$migration_file"
}

# List migrations
list_migrations() {
    log_info "Available migrations:"
    for f in "$MIGRATIONS_PATH"/*.sql; do
        echo "  $(basename $f)"
    done
}

# Check migration status (basic - checks if tables exist)
check_status() {
    log_info "Checking migration status..."

    # Check for key tables from each migration
    tables=(
        "products:20260128000001"
        "customers:20260128000001"
        "orders:20260128000001"
        "user_roles:20260129000001"
    )

    for item in "${tables[@]}"; do
        table="${item%%:*}"
        migration="${item##*:}"

        exists=$(docker exec $DB_CONTAINER psql -U postgres postgres -tAc "SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = '$table');")

        if [ "$exists" = "t" ]; then
            echo -e "  ${GREEN}[APPLIED]${NC} $migration - table '$table' exists"
        else
            echo -e "  ${YELLOW}[PENDING]${NC} $migration - table '$table' not found"
        fi
    done
}

# Main
case "${1:-help}" in
    apply-all)  apply_all ;;
    apply)      apply_one "$2" ;;
    list)       list_migrations ;;
    status)     check_status ;;
    *)
        echo "Migration Management Script"
        echo ""
        echo "Usage: $0 <command> [args]"
        echo ""
        echo "Commands:"
        echo "  apply-all           Apply all migrations in order"
        echo "  apply <file>        Apply specific migration"
        echo "  list                List available migrations"
        echo "  status              Check migration status"
        exit 1
        ;;
esac
```

Make script executable.
  </action>
  <verify>bash -n production/apply-migrations.sh && chmod +x production/apply-migrations.sh && echo "Script valid"</verify>
  <done>apply-migrations.sh provides commands to apply migrations to production with backup and status checking</done>
</task>

<task type="auto">
  <name>Task 2: Create production setup documentation</name>
  <files>docs/PRODUCTION_SETUP.md</files>
  <action>
Create docs/PRODUCTION_SETUP.md with complete production configuration guide.

```markdown
# Production Setup Guide

Complete guide for setting up the production Supabase deployment.

## Prerequisites

Before starting, ensure you have:
- [ ] Proxmox VM created and configured (see `docs/PROXMOX_VM_SETUP.md`)
- [ ] Docker installed and running on VM
- [ ] Supabase files in `/opt/supabase/`
- [ ] Production files copied from `production/` directory
- [ ] Google Cloud project for OAuth

## Quick Start

If all prerequisites are met:

```bash
# 1. SSH into VM
ssh user@<VM_IP>

# 2. Generate and configure secrets
cd /opt/supabase
./generate-secrets.sh > secrets.txt
# Copy secrets to .env

# 3. Configure .env
cp .env.template .env
nano .env  # Edit with generated secrets and URLs

# 4. Configure Google OAuth (see OAuth section below)

# 5. Deploy services
./deploy.sh start

# 6. Deploy Caddy
docker compose -f docker-compose.caddy.yml up -d

# 7. Apply migrations
./apply-migrations.sh apply-all

# 8. Enable Auth Hook (see Auth Hook section below)

# 9. Verify
curl https://supabase.edwardstech.dev/rest/v1/
```

## Step-by-Step Configuration

### 1. Generate Secrets

```bash
cd /opt/supabase
./generate-secrets.sh
```

Copy the output to a secure location. You'll need:
- `POSTGRES_PASSWORD`
- `JWT_SECRET`
- `SECRET_KEY_BASE`
- `VAULT_ENC_KEY`
- `LOGFLARE_*` tokens

### 2. Generate API Keys

ANON_KEY and SERVICE_ROLE_KEY are JWTs signed with your JWT_SECRET.

**Option A: Online Generator**
1. Go to: https://supabase.com/docs/guides/self-hosting/docker#generate-api-keys
2. Enter your JWT_SECRET
3. Copy the generated ANON_KEY and SERVICE_ROLE_KEY

**Option B: Using Supabase CLI (requires Node.js)**
```bash
npx supabase gen keys --jwt-secret <your-jwt-secret>
```

### 3. Configure .env

Copy the template and edit:

```bash
cp .env.template .env
nano .env
```

Required fields:
```bash
# Secrets (from step 1 and 2)
POSTGRES_PASSWORD=<generated>
JWT_SECRET=<generated>
ANON_KEY=<generated>
SERVICE_ROLE_KEY=<generated>
SECRET_KEY_BASE=<generated>
VAULT_ENC_KEY=<generated>

# URLs (for edwardstech.dev)
SUPABASE_PUBLIC_URL=https://supabase.edwardstech.dev
API_EXTERNAL_URL=https://supabase.edwardstech.dev
SITE_URL=https://www.edwardstech.dev

# OAuth (from Google Cloud Console)
GOTRUE_EXTERNAL_GOOGLE_ENABLED=true
GOTRUE_EXTERNAL_GOOGLE_CLIENT_ID=<your-client-id>
GOTRUE_EXTERNAL_GOOGLE_SECRET=<your-client-secret>
GOTRUE_EXTERNAL_GOOGLE_REDIRECT_URI=https://supabase.edwardstech.dev/auth/v1/callback
```

### 4. Configure DNS

Before Caddy can get SSL certificates, DNS must be configured.

**In Cloudflare (or your DNS provider):**

1. Add A record:
   - Type: A
   - Name: supabase
   - Content: <VM_IP>
   - Proxy status: DNS only (grey cloud) OR Proxied (orange cloud)

2. (Optional) Add A record for Studio:
   - Type: A
   - Name: studio
   - Content: <VM_IP>

**If using Cloudflare Proxy (orange cloud):**
- Set SSL/TLS mode to "Full (strict)"
- Caddy will still get certificates (for origin connection)

**Verify DNS:**
```bash
dig supabase.edwardstech.dev
# Should return your VM's IP
```

### 5. Configure Google OAuth

**Create Production OAuth Credentials:**

1. Go to [Google Cloud Console](https://console.cloud.google.com/)
2. Select your project (or create new one for production)
3. Navigate to: APIs & Services → Credentials
4. Click: Create Credentials → OAuth client ID
5. Application type: Web application
6. Name: Edwards Engineering Production
7. Authorized JavaScript origins:
   - `https://www.edwardstech.dev`
   - `https://supabase.edwardstech.dev`
8. Authorized redirect URIs:
   - `https://supabase.edwardstech.dev/auth/v1/callback`
9. Click Create and copy Client ID and Secret

**Add to .env:**
```bash
GOTRUE_EXTERNAL_GOOGLE_CLIENT_ID=<client-id>.apps.googleusercontent.com
GOTRUE_EXTERNAL_GOOGLE_SECRET=<client-secret>
```

### 6. Deploy Services

```bash
# Verify environment
./deploy.sh env-check

# Start Supabase
./deploy.sh start

# Wait for services (30-60 seconds)
./deploy.sh status

# Start Caddy (after DNS is configured)
docker compose -f docker-compose.caddy.yml up -d

# Check Caddy logs for certificate status
docker logs caddy
```

### 7. Apply Migrations

```bash
# Check migration status
./apply-migrations.sh status

# Apply all migrations
./apply-migrations.sh apply-all
```

### 8. Enable Auth Hook

The custom_access_token_hook (for role injection) must be registered manually in Supabase Studio.

1. Access Studio at: https://studio.edwardstech.dev (or via Supabase API if Studio is disabled)
2. Navigate to: Authentication → Hooks
3. Enable: Custom Access Token
4. Select function: `public.custom_access_token_hook`
5. Save

**Alternative via SQL:**
```sql
-- Run in Supabase SQL Editor or via psql
UPDATE auth.hooks
SET enabled = true
WHERE hook_name = 'custom_access_token_hook';
```

### 9. Create Admin User

After first login via Google OAuth, assign admin role:

```sql
-- In Supabase Studio SQL Editor
INSERT INTO public.user_roles (user_id, role)
SELECT id, 'admin'
FROM auth.users
WHERE email = 'your-admin-email@gmail.com';
```

## Verification Checklist

After setup, verify:

- [ ] `curl https://supabase.edwardstech.dev/rest/v1/` returns empty array or data
- [ ] SSL certificate is valid (check browser padlock)
- [ ] Google OAuth login works from frontend
- [ ] Admin can access dashboard
- [ ] Database tables exist (check via Studio)
- [ ] Backup service is running: `docker ps | grep backup`

## Environment Parity

### Local vs Production Differences

| Aspect | Local (CLI) | Production (Docker) |
|--------|-------------|---------------------|
| Start command | `npx supabase start` | `./deploy.sh start` |
| URL | `http://localhost:54321` | `https://supabase.edwardstech.dev` |
| Studio | `http://localhost:54323` | `https://studio.edwardstech.dev` |
| Migrations | Auto-applied on start | Manual: `./apply-migrations.sh` |
| Auth Hook | Via Studio UI | Via Studio UI |
| OAuth | Separate dev credentials | Production credentials |

### Sharing Configuration

To keep local and production aligned:
- Migrations: Same SQL files applied to both
- Schema: Track in git, apply to both environments
- Auth hooks: Must be registered separately in each environment

## Troubleshooting

### Services Won't Start

```bash
# Check logs
./deploy.sh logs

# Common issues:
# - .env missing required values
# - Port conflicts
# - Docker network issues
```

### SSL Certificate Not Issued

```bash
# Check Caddy logs
docker logs caddy

# Common issues:
# - DNS not propagated (wait or check with dig)
# - Port 80 blocked
# - Rate limited by Let's Encrypt
```

### OAuth Not Working

- Verify redirect URI matches EXACTLY (including https)
- Check SITE_URL and API_EXTERNAL_URL in .env
- Verify Google OAuth credentials are for production (not local)

### Database Connection Issues

```bash
# Test connection
docker exec supabase-db psql -U postgres -c "SELECT 1"

# Check logs
docker logs supabase-db
```
```
  </action>
  <verify>grep -c "OAuth" docs/PRODUCTION_SETUP.md (should be >= 5)</verify>
  <done>PRODUCTION_SETUP.md provides complete production configuration guide with OAuth and verification steps</done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <what-built>Complete production infrastructure: VM, Supabase stack, Caddy SSL, backups, and configuration</what-built>
  <how-to-verify>
**Verification Steps:**

1. **HTTPS Access:**
   - Visit https://supabase.edwardstech.dev/rest/v1/
   - Should return `[]` or data (not connection error)
   - SSL certificate should be valid (green padlock)

2. **Health Check:**
   ```bash
   # On VM
   ./deploy.sh status
   # All services should show "healthy"

   # API health
   curl -s https://supabase.edwardstech.dev/rest/v1/ | head
   ```

3. **OAuth Login:**
   - From frontend (local or deployed), attempt Google login
   - Should redirect to Google, then back to app
   - User should be authenticated

4. **Admin Access:**
   - After OAuth login, assign admin role via SQL
   - Access admin dashboard
   - Verify protected routes work

5. **Backup Verification:**
   ```bash
   # On VM
   docker ps | grep backup
   # Should show supabase-db-backup running

   # Create test backup
   ./backup.sh backup
   ./backup.sh list
   ```

**Expected Results:**
- API accessible via HTTPS with valid SSL
- OAuth flow completes successfully
- Admin can access protected routes
- Backup service running and creating backups
  </how-to-verify>
  <resume-signal>Type "verified" if all checks pass, or describe specific issues encountered</resume-signal>
</task>

</tasks>

<verification>
- [ ] apply-migrations.sh is executable and passes syntax check
- [ ] PRODUCTION_SETUP.md covers all configuration steps
- [ ] Production Supabase is accessible via HTTPS
- [ ] OAuth authentication works
- [ ] Admin can access protected routes
- [ ] Backup service is running
</verification>

<success_criteria>
- Supabase API is accessible at https://supabase.edwardstech.dev
- SSL certificate is valid and auto-renewed
- Google OAuth login works for admin
- Database contains migrated schema and data
- Automated backups are running daily
- Frontend can connect to production Supabase
</success_criteria>

<output>
After completion, create `.planning/phases/04-production-infrastructure/04-06-SUMMARY.md`
</output>
