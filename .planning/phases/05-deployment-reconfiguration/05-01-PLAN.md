---
phase: 05-deployment-reconfiguration
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - Dockerfile.backend
  - docker-compose.backend.yml
  - contact-backend/.env.production.template
autonomous: true

must_haves:
  truths:
    - "Backend container builds successfully with all Supabase source files included"
    - "Backend container runs with health check responding on port 3001"
    - "Docker Compose config exposes backend to LAN on 0.0.0.0:3001"
  artifacts:
    - path: "Dockerfile.backend"
      provides: "Production backend container with health check"
      contains: "HEALTHCHECK"
    - path: "docker-compose.backend.yml"
      provides: "Backend deployment config for Proxmox VM"
      contains: "0.0.0.0:3001:3001"
    - path: "contact-backend/.env.production.template"
      provides: "Production environment variable template for backend on VM"
      contains: "SUPABASE_URL"
  key_links:
    - from: "docker-compose.backend.yml"
      to: "Dockerfile.backend"
      via: "image reference"
      pattern: "192.168.0.40:5000/backend"
    - from: "docker-compose.backend.yml"
      to: "contact-backend/.env.production.template"
      via: "env_file reference"
      pattern: "env_file"
---

<objective>
Update the backend Dockerfile and create Docker Compose configuration for deploying the Express backend to the Proxmox VM.

Purpose: The backend must run on the Proxmox VM (not the Pi cluster) to prevent SD card wear from database connections and stateful operations. The existing Dockerfile.backend is missing health checks, the src/ directory (containing Supabase config, middleware, services), and production best practices.

Output: Updated Dockerfile.backend with all source files and health check, docker-compose.backend.yml for VM deployment, and a production environment template.
</objective>

<execution_context>
@/home/logan/.claude/get-shit-done/workflows/execute-plan.md
@/home/logan/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-deployment-reconfiguration/05-RESEARCH.md

@Dockerfile.backend
@contact-backend/server.js
@contact-backend/package.json
@contact-backend/src/config/supabase.js
@contact-backend/src/lib/supabase-ssr.js
@contact-backend/src/middleware/auth.js
@contact-backend/src/middleware/requireAdmin.js
@contact-backend/src/services/database.js
</context>

<tasks>

<task type="auto">
  <name>Task 1: Update Dockerfile.backend with complete source tree and health check</name>
  <files>Dockerfile.backend</files>
  <action>
Update the existing Dockerfile.backend to include ALL backend source files (the current Dockerfile only copies server.js, database.js, and pdf-generator.js but misses the entire src/ directory added during Phases 2-3).

Requirements:
1. Keep FROM node:18-alpine base image
2. Keep the non-root user setup (addgroup/adduser pattern already present)
3. Keep the dependency layer caching (COPY package*.json then npm ci)
4. Update the COPY commands to include:
   - contact-backend/server.js
   - contact-backend/database.js
   - contact-backend/pdf-generator.js
   - contact-backend/src/ (entire directory - contains config/, lib/, middleware/, services/)
5. Add NODE_ENV=production environment variable
6. Add HEALTHCHECK instruction:
   - interval: 30s, timeout: 3s, start-period: 40s, retries: 3
   - Command: node -e "require('http').get('http://localhost:3001/api/health', (r) => {process.exit(r.statusCode === 200 ? 0 : 1)})"
7. Keep EXPOSE 3001 and CMD ["npm", "start"]
8. Keep chown and USER backend lines

Do NOT change the build context assumption - the Dockerfile is in repo root and copies from contact-backend/ relative path (this matches the existing pattern and the build-and-push.sh script that uses -f Dockerfile.backend with repo root as context).
  </action>
  <verify>
Run `docker build -f Dockerfile.backend -t test-backend:verify .` from repo root. Build should complete without errors. Verify with `docker inspect test-backend:verify --format='{{json .Config.Healthcheck}}'` to confirm health check is present. Clean up with `docker rmi test-backend:verify`.
  </verify>
  <done>Dockerfile.backend builds successfully, includes src/ directory, has HEALTHCHECK instruction, and maintains non-root user security pattern.</done>
</task>

<task type="auto">
  <name>Task 2: Create Docker Compose backend config and production env template</name>
  <files>docker-compose.backend.yml, contact-backend/.env.production.template</files>
  <action>
Create docker-compose.backend.yml in the repo root for deploying the backend on the Proxmox VM.

docker-compose.backend.yml requirements:
1. version: '3.8'
2. Service name: backend
3. image: 192.168.0.40:5000/backend:${GIT_SHA:-latest}
4. container_name: edwards-backend
5. restart: unless-stopped
6. env_file: .env (will be copied from .env.production.template)
7. ports: "0.0.0.0:3001:3001" (MUST bind to 0.0.0.0, NOT 127.0.0.1 - otherwise k3s cluster cannot reach it over LAN)
8. healthcheck matching the Dockerfile HEALTHCHECK (test, interval, timeout, retries, start_period)
9. logging: json-file driver, max-size 10m, max-file 3
10. deploy.resources: limits cpus '2', memory 512M; reservations cpus '0.5', memory 256M

Create contact-backend/.env.production.template with inline documentation:
1. Supabase section: SUPABASE_URL (https://supabase.edwardstech.dev), SUPABASE_ANON_KEY, SUPABASE_SERVICE_ROLE_KEY
2. Application section: NODE_ENV=production, PORT=3001, FRONTEND_URL=https://edwardstech.dev
3. Email section: EMAIL_USER, EMAIL_APP_PASSWORD
4. Stripe section: STRIPE_SECRET_KEY, STRIPE_WEBHOOK_SECRET
5. Add comment: "# Legacy DB vars no longer needed - backend uses Supabase client"
6. Add comment at top: "# Copy to .env and fill in actual values"
7. Do NOT include DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD - backend now uses Supabase client (migrated in Phase 2)
8. Do NOT include JWT_SECRET, ADMIN_USER, ADMIN_PASS - auth now uses Supabase Auth (migrated in Phase 3)
  </action>
  <verify>
Run `docker compose -f docker-compose.backend.yml config` to validate the compose file syntax. Verify the env template contains SUPABASE_URL and does NOT contain DB_HOST or JWT_SECRET.
  </verify>
  <done>docker-compose.backend.yml is valid, binds to 0.0.0.0:3001, references registry image, has health check and resource limits. Production env template documents all required Supabase-era environment variables without legacy database/JWT variables.</done>
</task>

</tasks>

<verification>
1. `docker build -f Dockerfile.backend -t test-backend:verify .` completes successfully
2. `docker compose -f docker-compose.backend.yml config` shows valid configuration
3. Dockerfile.backend includes COPY for src/ directory
4. Dockerfile.backend includes HEALTHCHECK instruction
5. docker-compose.backend.yml binds to 0.0.0.0:3001 (not 127.0.0.1)
6. contact-backend/.env.production.template contains Supabase vars, no legacy DB vars
</verification>

<success_criteria>
- Backend container builds with all Supabase source files (src/config, src/lib, src/middleware, src/services)
- Health check endpoint (/api/health) is configured in both Dockerfile and Compose
- Docker Compose exposes port 3001 on all interfaces for LAN access
- Production env template reflects Supabase-migrated backend (no legacy DB/JWT vars)
</success_criteria>

<output>
After completion, create `.planning/phases/05-deployment-reconfiguration/05-01-SUMMARY.md`
</output>
